% logistic regression with pca

%clc;
clearvars;
close all;

rng(0); % random seed

sim.save_figures = 0
sim.plot_things = 0


% hyper-parameters

sim.penalty = 0.001
sim.learning_rate = 0.001
sim.iterations = 100

sim.train_ratio = 0.80




% added

sim.pca = 1

for pca_count = 1:13


sim.pca_count = pca_count;

[~,accuracies] = train_and_test(sim);

hold on
subplot(4,4,pca_count)
if sim.pca==1
    with_pca_title = ["Accuracy (with PCA)" num2str(sim.pca_count)];
plot(accuracies), title(with_pca_title), xlabel('Iteration'), xlim([0,sim.iterations]), ylim([0,1])
else
plot(accuracies), title("Accuracy (without PCA)"), xlabel('Iteration'), xlim([0,sim.iterations]), ylim([0,1])

end
hold off

end


function [W, accuracies, losses] = train_and_test(sim)
    
labels = {'blues', 'classical', 'country', 'disco', 'hiphop', ...
          'jazz', 'metal', 'pop', 'reggae', 'rock'};



% load training data
in_csv = '../in/mfcc_13_labels.csv'; 

table = readtable(in_csv);


X = table2array(table(:, 1:13));
Y_labels = table2array(table(:, 14));

% standardize features
X = zscore(X);  
% same: X = (X - mean(X)) ./ std(X)  

if sim.pca==1   % with pca

    % 1. Center columns
    X2 = X - mean(X, 1); 
    
    % 2. Build covariance matrix
    cov_mat = cov(X2); % 13 x 13
    
    % 3. Eigenvectors and eigenvalues
    [eig_vecs, eig_vals] = eig(cov_mat);
    
    % 4. Sort eigenvectors highest to lowest
    [~,idx] = sort(diag(eig_vals), 'descend');
    eig_vecs_sorted = eig_vecs(:,idx);
    
    % 5. Use the top k eigenvectors
    k=sim.pca_count;  % from cumsum >= 0.95
    projection_mat = eig_vecs_sorted(:,1:k);
    
    % 6. Project onto centered columns to reduce
    X_reduced = X2 * projection_mat;
    
    X = X_reduced;

end

% one hot encode labels
num_labels = size(unique(Y_labels), 1);
num_samples = size(Y_labels, 1);



Y_ids = zeros(size(Y_labels));  % not used yet
for i = 1:num_samples
    Y_ids(i) = find(strcmp(labels, Y_labels{i}));   % position in labels array
end


Y = zeros(num_labels, num_samples);
for i = 1:num_samples
    index = find(strcmp(labels, Y_labels{i}));
    Y(index, i) = 1;
end






  


    % column of ones
    X = [ones(num_samples, 1) X];



    

    
    
    
    % split data into training and testing sets
   % [X_train, X_test, Y_train, Y_test] = train_test_split(X, Y, sim.train_ratio);

num_train = round(sim.train_ratio * num_samples);

% Randomly shuffle indices without replacement
%indices = randperm(size(X, 1));

% Randomly shuffle indices with replacement
indices = randi(size(X, 1), 1, size(X, 1));
    
% Split
X_train = X(indices(1:num_train), :);
X_test = X(indices(num_train+1:end), :);
Y_train = Y(:,indices(1:num_train));
Y_test = Y(:,indices(num_train+1:end));             

W = rand(num_labels, size(X_train, 2));    % random gives a 0.01 accuracy increase

sigmoid = @(s) 1./(1 + exp(-s));

losses = zeros(sim.iterations, 1);
accuracies = zeros(sim.iterations, 1);


for j = 1:sim.iterations

    
    PY = sigmoid(W*X_train'); 
    
    W = W + sim.learning_rate * ((Y_train - PY) * X_train - sim.penalty * W);

    % Make prediction
    
    [~, argmax] = max(sigmoid(W*X_test'), [], 1);   % row max
    
    % Y_test one hot encoded
    % argmax are numbers

    score = 0;
    for i = 1:size(Y_test, 2)
        if Y_test(argmax(i), i) == 1

            score = score + 1;
        end
    end
    
    accuracy = score / size(Y_test, 2)


    accuracies(j) = accuracy;
    %losses(j) = mean(Y - PY, "all");
end

end
