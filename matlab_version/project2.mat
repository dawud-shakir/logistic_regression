% project 2 (matlab) todo: python version 

% logistic regression math: https://web.stanford.edu/~jurafsky/slp3/5.pdf


% in matlab
%rows = 1
%cols = 2    


%clc;
clearvars;
close all;

rng(0); % random seed

% hyper-parameters


%sim.regularization = 0.001
sim.error_tolerance = 0.001
sim.learning_rate = 0.001
sim.iterations = 10000

sim.train_ratio = 0.80; 


sim.save_figures = 0
sim.plot_things = 0



% load training data
in_csv = './in/mfcc_13_ids.csv'; 
df = readtable(in_csv);

%read number of coefficients from file name
num_coefficients = regexp(in_csv, '\d+', 'match');
num_coefficients = str2double(num_coefficients);
sim.num_coefficients = num_coefficients

sim.sigmoid = @(s) 1 ./ (1 + exp(-s))


% split data into features (X) and labels (Y)
X = table2array(df(:, 1:13));
Y = table2array(df(:, 14));

% train
[W, accuracies, losses] = train_model(X, Y, sim);

disp(['Mean Accuracy: ', num2str(mean(accuracies))])
disp(['Min Accuracy: ', num2str(min(accuracies))])
disp(['Max Accuracy: ', num2str(max(accuracies))])
disp(['Std Accuracy: ', num2str(std(accuracies))])





% test model and output kaggle file

df = readtable('./in/mfcc_13_kaggle.csv');




% split data into features (X) and labels (Y)
X_test = table2array(df(:, 1:13));
% standardize features
X_test = (X_test-mean(X_test)) ./ std(X_test);  % X = zscore(X_test)
% add column of ones
X_test = [ones(size(X_test, 1), 1) X_test];   


Y_predicted = test_model(X_test, W, sim);
%sim.score =  score(Y_test, Y_predicted, sim);


column_class = cell(numel(Y_predicted, 2));
column_id = {dir("./data/test/*.au").name};

for i = 1:size(Y_predicted,2)
    
    column_class{i} = num2class(Y_predicted(i));

end




T = table(column_id', column_class', 'VariableNames', {'id', 'class'});

out_file = "./out/2_mfcc_13_test.csv";
if exist(out_file, 'file') == 2
    error(sprintf('file error: %s already exists!', out_file));
else
    disp(['outfile: ', out_file])
    writetable(T, out_file)
end

if sim.plot_things==1
    if sim.save_figures==1
        plot_scatter(X, Y, sim)
        savefig('scatter.fig')
        plot_losses(losses, sim);
        savefig('scatter.fig')
        
        plot_accuracies(accuracies, sim);
        savefig('scatter.fig')
    
    else
        plot_scatter(X, Y, sim)
        plot_losses(losses, sim);

        plot_accuracies(accuracies, sim);
    end

end


function class = num2class(number)


switch number
    case 1
        class = "blues"; 
    case 2 
        class = "classical"; 
    case 3 
        class = "country";
    case 4 
        class = "disco";
    case 5 
        class = "hiphop";
    case 6 
        class = "jazz";
    case 7 
        class = "metal";
    case 8
        class = "pop";
    case 9 
        class = "reggae";
    case 10 
        class = "rock";
end

end

% a vector with one value=1 and the rest value=0 
function Y = one_hot_encoding(Y_labels)

    K = numel(unique(Y_labels));
    M = size(Y_labels, 1);
    
    Y = zeros(K,M);
    for j = 1:M
        Y(Y_labels(j), j) = 1; 
    end

end

function [W, accuracies, losses] = train_model(X, Y, sim)
    K = numel(unique(Y));
    N = size(X,2);
    M = size(X,1);

    sigmoid = @(s) 1./(1 + exp(-s));


    % standardize features
    X = (X-mean(X)) ./ std(X);  % X = zscore(X)
    
    if sim.plot_things==1
        plot_zscores(X,Y,sim);
    end

    
    
    % split data into training and testing sets
    train_split = round(sim.train_ratio * size(X, 1));
    
    % sample randomly
    %all_samples = randperm(M); % without replacement
    all_samples = randi(M, 1, M); % with replacement
    
    
    train_indices = all_samples(1:train_split);
    test_indices = all_samples(train_split+1:end);
    
    X_train = X(train_indices, :);
    Y_train = one_hot_encoding(Y(train_indices, :));
    
    X_test = X(test_indices, :);
    Y_test = Y(test_indices, :);
    
    % add biases (intercept)
    X_train = [ones(size(X_train, 1), 1) X_train];   %X0
    X_test = [ones(size(X_test, 1), 1) X_test];   %X0
    

    losses = zeros(1, M);
    accuracies = zeros(1, M);


    W = rand(K, N+1);    % random is a 0.01 accuracy increase

    for i = 1:sim.iterations
    
        Y_predicted = sigmoid(W*X_train'); 
        
        % Accuracy: 0.54444
        %Y_predicted = exp(W*X'); % predictions
        %Y_predicted = Y_predicted ./ sum(Y_predicted, 1); % normalize (0.1111 without)
        
        gradient = ((Y_train-Y_predicted) * X_train - sim.error_tolerance * W);
    
        W = W + sim.learning_rate * gradient;
    
        Y_prediction = test_model(X_test, W, sim);
        accuracies(i) = score(Y_test, Y_prediction, sim);

        % cross-entropy loss
        losses(i) = -sum(sum(Y_train .* log2(Y_predicted)));    % all entropy summed

    end


end


function Y_prediction = test_model(X_test, W, sim)
    sigmoid = @(s) 1./(1 + exp(-s));
    
    Y_model = sigmoid(W*X_test');
        
        
    %
    %Y_model = exp(W*X_test'); % predicted probabilities
    %Y_model = Y_model ./ sum(Y_model, 1); % normalize
    
    

    [~, Y_prediction] = max(Y_model, [], 1) ; % argmax
    
    % regularize
    %Y_prediction = Y_prediction - regularization*sum(W,2);    % L1
  %  Y_prediction = Y_prediction - regularization*sum(W,2).^2; % L2

end

function accuracy = score(Y_test, Y_prediction, sim)

accuracy = sum(Y_prediction == Y_test') / numel(Y_test);
disp(['Accuracy: ', num2str(accuracy)]);
end


function plot_scatter(X,Y,sim)
figure;
% plot data points
hold on;

gscatter(X,Y)

xlabel("Data");
ylabel("Class");
title('Scatter');

grid on;
hold off;

end


function plot_accuracies(accuracies,sim)
figure;
% plot data points
hold on;

plot(accuracies);

xlabel("Iteration");
ylabel("Accuracy");
title("Accuracy");

grid on;
hold off;
    
    
end

function plot_zscores(X,Y,sim)
figure;
% plot data points

gscatter(X, Y);

hold on;

xlabel('Z-Score $\frac{x-mean(x)}{std(x)}$', 'interpreter', 'latex');
ylabel('Folder');
title('Data Points');

grid on;
hold off;

end


function plot_losses(losses,sim)
figure;

plot(losses);

hold on;

xlabel('Iteration');
ylabel('Cross-entropy loss');
title('Loss');

grid on;
hold off;



end





% scratch area ... enter at your on risk ...  ☠️☠️☠️
%{


% for regularization
costs = zeros(1, iterations);
costs_L1 = zeros(1, iterations);
costs_L2 = zeros(1, iterations);



    % Update weights using gradient descent
    % For L1 regularization
%%    W = W - (learning_rate * (1/m) * ((Y_pred - Y') * X + lambda * sign(W)));
    
    % For L2 regularization
%%    W = W - (learning_rate * (1/m) * ((Y_pred - Y') * X + lambda * W));

    %cost = -(1/M) * sum(Y_actual .* log(Y_k) + (1 - Y_actual) .* log(1 - Y_k));
  
    %cost_L1 = cost + epsilon * sum(abs(W(:,2:end))); % Exclude bias term from regularization
    %cost_L2 = cost + epsilon * sum(W(:,2:end).^2); % Exclude bias term from regularization

    %costs(i) = cost;
    %costs_L1(i) = cost_L1;
    %costs_L2(i) = cost_L2;









figure;
% plot data points
hold on;

plot(accuracies);
%plot(costs_L1);
%plot(costs_L2);


xlabel("Iteration");
ylabel("Accuracy");
title('Accuracy');

grid on;
hold off;


%}

